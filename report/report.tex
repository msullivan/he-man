\documentclass[preprint,11pt]{sigplanconf}
\usepackage[colorlinks=true,linkcolor=black,urlcolor=black,citecolor=black]%
{hyperref}
\usepackage{graphicx}
\renewcommand{\t}{\texttt}
\renewcommand{\b}{\textbf}
\renewcommand{\i}{\textit}

\newcommand{\prettybox}[1]{\fbox{\parbox{\linewidth}{#1}}}

% A bunch of crazy BNF TeX I wrote once
\def\bnf#1{\prettybox{\halign{##\hfil& \hfil ##\hfil& \ ##\hfil\cr #1}}}
\def\p#1{& = & #1 \cr}
\def\pp#1{& & #1 \cr}
\def\a#1{& $\mid$ & #1 \cr}

\title{He-Man: A domain-specific language for lightweight concurrency}
%\subtitle{I said hey, what's goin' on}
\authorinfo{Carlo Angiuli}{}{cangiuli@cs.cmu.edu}
\authorinfo{Michael Sullivan}{}{mjsulliv@andrew.cmu.edu}
\date{December 12, 2011}

\begin{document}
\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Introduction}

Network applications need to support many simultaneous clients---this is known
as the C10K (``10,000 client'') problem \cite{Kegel}. Modern servers have enough
memory and bandwidth for massive concurrency, but it is difficult to design
applications to take full advantage of these resources.

A server must maintain separate state for each connection, and often waits for
network or disk I/O to complete before continuing to serve a particular client.
The simplest solution is to maintain one thread per client, using blocking I/O
within each thread. This incurs frequent and expensive context switches at the
operating system level; because the threads have identical code and little
private data, this approach is unnecessary and does not scale.

An improvement is to write multithreaded code, in which each thread serves
multiple clients. This is the most popular approach, and is utilized in the
Apache HTTP server \cite{Apache}.

Languages like Erlang and Haskell provide lightweight threads via user-space
schedulers. This affords even better performance, but remains unnecessarily
powerful for this problem space, imposing runtime overhead on applications which
don't require preemptable, independently scheduled threads \cite{Vinoski}.

By far the leanest approach is to write an event loop around kernel facilities
for multiplexed, non-blocking I/O, such as \t{select}, \t{poll}, \t{epoll}, or
\t{kquery}. In this model, all blocking occurs at a single call site in the code.
The programmer is responsible for explicitly maintaining per-client state and
invoking the appropriate client on each I/O event.

This requires the entire application to be restructured as a state machine with
explicit per-client control blocks, a model which is very difficult for
programmers to reason about. There exist numerous C/C++ libraries for event
loops, but none of these avoid the state machine paradigm.

Our observation is that the event loop model, in programming languages parlance,
is essentially trampolining continuation-passing-style, which can be obtained by
mechanical translation from the straight-line threaded code. This suggests it is
possible to obtain the best of both worlds---a threaded programming model with
the performance of event loops. 

To that end, we present He-Man: Haskell Event Manager, Apropos Networking, an
embedded domain-specific language (EDSL) hosted in Haskell, which provides a
threaded abstraction for network application programming and compiles to a C
event loop based around Linux's \t{epoll}.

He-Man is a straightforward monadic language with built-in network primitives,
offering a simple thread-per-client programming model. It is easily extensible
in Haskell, and also allows calls to foreign C functions; it outputs C code
which links with an event loop runtime and any code a programmer might prefer to
write directly in C. Our vision is that C programmers can use He-Man for their
core application logic, and can write the rest of their application in C. 

\section{Related work}

The C10K problem is particularly evident in the context of HTTP servers, which
routinely maintain many concurrent connections. As we discuss in Section
\ref{sec:evaluation}, Apache does not scale particularly well. The popular
Lighttpd and Nginx HTTP server projects were both started with the explicit goal
of outperforming Apache with respect to the C10K problem \cite{Lighttpd,Nginx},
and both use event-based architectures similar to He-Man. 

A number of functional languages support lightweight threads with user-space
scheduling, most notably Erlang, which was designed for highly concurrent
telecommunications applications, and easily scales to 20,000 clients
\cite{Hellstrom}. The Haskell runtime supports a variety of concurrency
primitives which permit a lightweight threaded programming model \cite{LiEtAl}.

%A number of custom network stack implementations exist for functional
%languages, such as FoxNet for Standard ML \cite{BiagioniEtAl}, and 

He-Man is most similar to Li and Zdancewic's Haskell implementation of a hybrid
event/thread programming model for network applications \cite{LiZdancewic}.
Their project has similar goals to ours, but is implemented entirely within
Haskell. As a result, unlike He-Man, the entire application must be written in
Haskell, and incurs runtime overhead from the Haskell system.

\section{The event loop model}



Event loops are a good way to get the level of concurrency that you need

we don't need arbitrary interleaving of instructions, 


there shouldn't be much

I/O time (both network and disk) dominates the computation time

thus it's important to run short computations until a thread blocks, 

The back-end converts our linear front-end code into a collection of states 

is to compile C, 

It's important that the only blocking operations happen in the main loop

We separate 

Threads are separated and replaced by labels

pass to make it 



\section{Implementation}

\subsection{Front-end}

\begin{figure}[ht]
\centering
\bnf{
\i{Stmt}
\p{\b{Decl} (\i{Var}, \i{Type}) \i{Expr}}
\a{\b{While} \i{Expr} [\i{Stmt}]}
\a{\b{If} \i{Expr} [\i{Stmt}] [\i{Stmt}]}
\a{\b{Spawn} ([(\i{Var}, \i{Type})], [\i{Stmt}]) [\i{Expr}]}
\a{\b{Assign} \i{Expr} \i{Expr}}
\a{\b{Exp} \i{Expr}}
\a{\b{Wait} \i{Expr}}
\a{\b{Exit}}
\i{Expr}
\p{\b{Call} \i{String} [\i{Expr}]}
\a{\b{Arith} \i{ArithOp} \i{Expr} \i{Expr}}
\a{\b{ArithUnop} \i{ArithUnop} \i{Expr}}
\a{\b{RelnOp} \i{RelnOp} \i{Expr} \i{Expr}}
\a{\b{Constant} \i{String}}
\a{\b{NumLit} \i{Integer}}
\a{\b{StringLit} \i{String}}
\a{\b{Var} \i{String}}
\a{\b{CurThread}}
\i{Type} 
\p{\b{Int} $\mid$ \b{Bool} $\mid$ \b{String}
   $\mid$ \b{Buffer} $\mid$ \b{Event}}
}
\caption{Front-end grammar.}
\end{figure}

\subsection{Back-end}

The back-end converts our linear front-end code into a collection of states 

is to compile C, 

It's important that the only blocking operations happen in the main loop

We separate 

Threads are separated and replaced by labels

pass to make it 

\begin{figure}[ht]
\centering
\bnf{
\i{Thread}
\p{(\i{thread\_label}, [(\i{Var}, \i{Type})])}
\i{Block}
\p{(\i{label}, \i{thread\_label}, [\i{Stmt}], \i{Tail})}
\i{Tail}
\p{\b{Goto} \i{label}}
\a{\b{GotoWait} \i{label}}
\a{\b{If} \i{Expr} \i{Tail} \i{Tail}}
\a{\b{Exit}}
\i{Stmt}
\p{\b{Decl} (\i{Var}, \i{Type}) \i{Expr}}
\a{\b{Assign} \i{Expr} \i{Expr}}
\a{\b{Exp} \i{Expr}}
\a{\b{Spawn} \i{label} [((\i{Var}, \i{Type}), \i{Expr})]}
}
\caption{Back-end grammar.}
\end{figure}

\subsection{Runtime}

\section{Evaluation}\label{sec:evaluation}

To evaluate He-Man, we built two network applications with He-Man and
compared them to similar network applications built in C.
We also discuss He-Man's memory overhead.

\subsection{Echo Server}

To compare the ease of writing event based network applications, we
built simple echo servers using both He-Man and C. The C version was
written in as an event loop driven explicit state machine using the
He-Man runtime. The C version, which is fairly tightly written, is 98
lines of source, according to SLOCCount. It is fairly readable, but
its structure is obscured by being split into states and the need to
explicitly handle read and write returning EAGAIN and write returning
short. The state machine has a total of five states. The He-Man
version is 33 lines long and is a very clear representation of the
control flow of each individual thread. (Additionally, the He-Man
library has 17 lines of code implementing generic routines for
simulating blocking IO using nonblocking IO and wait.) The total line
counts, however, somewhat understate the difference between the
versions, since the boilerplate for setting up connections is about
the same for both versions. When just looking at the core read/write
loop, the difference is much more apparent: the C version is 38 lines
of code, and the He-Man version is 5.

The generated code is substantially less tight than the C version,
although it is unlikely to be any slower in practice. It has 15 states
and is 319 lines of C. This could be reduced substantially by allowing
conditionals outside of block tail position.

\subsection{HTTP Server}

To test the suitability of our system for more complicated programs
and to demonstrate seperating network logic and application logic, we
built a simple (and bad) HTTP 1.0 server. The network logic is written
in He-Man and request parsing is written in C. The C side is
implemented as a function that takes a buffer and returns whether it
forms a complete HTTP request and, if so, fills a buffer with the
filename to serve. The entire server is 54 lines of He-Man haskell and
27 lines of C. It suffers from a number of major deficiencies, but is
complete enough to benchmark.

Our benchmark measurements are taken using ab, the Apache HTTP server
benchmarking tool. \cite{ApacheAB} Measurements were collected by
having a variable number of client threads request a file 100 times;
the experiment was repeated with a 16 kilobyte file and a 128 kilobyte
file. We ran the tests against both our He-Man http server and Apache
2.2.20, installed with the default Ubuntu 11.10 settings, except for
client, thread, and process limits being adjusted upwards. Since the
He-Man runtime is currently only single threaded, we ran the Apache
web server pinned to a single processor. Since the test repeatedly
requests the same file, it will always be in the kernel's disk cache,
and thus this test is not disk bound.

For our experiments, the server ran Linux kernel 3.0.0 on a Lenovo
Thinkpad W520 with a quad core 2.3 GHz Intel Core i7-2820QM and 8
gigabytes of RAM, and the client ran on a Lenovo Thinkpad T500 with a
Intel Core 2 Duo and 8 gigabtyes of RAM.  % XXX: client specs
The machines were connected by a gigabit Ethernet link.

Unfortunately, ab is not particularly scalable, so we had trouble
testing with more clients than 500.

Figure \ref{fig:graph128} compares our server to Apache for a 128 KB
file. Our server performs nearly identically to Apache on this load.
Figure \ref{fig:graph16} compares our server to Apache for a 16 KB
file. In this scenario, we substantially outperform Apache. We
conjecture this is because with a smaller file size, overhead for
connection setup begins to dominate, and connection setup in He-Man
consists of a few memory allocations whereas connection setup under
Apache may require spawning new OS level threads.

\begin{figure*}[!htb]
	\centering
	\includegraphics[width=0.9\textwidth]{128kb_graph.png}
	\caption{Web server benchmark, 128kb file}
	\label{fig:graph128}
\end{figure*}
\begin{figure*}[!htb]
	\centering
	\includegraphics[width=0.9\textwidth]{16kb_graph.png}
	\caption{Web server benchmark, 16kb file}
	\label{fig:graph16}
\end{figure*}


\subsection{Memory Overhead}

The per-thread memory overhead imposed by He-Man is very small: it
consists of a thread id, list pointers for the scheduling queue, the
next state to enter, a pointer to the most recently received event,
and list heads and tails for lists of the events being waited on,
events owned by the thread, and buffers owned by the thread (the
latter two are so that we can automatically reclaim these resources
when the thread exits). On a 64-bit system, the total overhead comes
out to 88 bytes, and could easily be reduced further (e.g. by using
singly linked lists).  For comparison, the default thread stack size
under NPTL on Linux is 2 megabytes.

say something about Memory.hs??


\section{Future work}

multithreaded runtime

write a better allocator

less space per thread

decide when to allow internal if statements

We will additionally output a graphical representation of this underlying state
machine.

observable sharing (?)

\section{Conclusions}

\bibliography{citations}{}
\bibliographystyle{abbrvnat}

\end{document}
