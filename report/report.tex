\documentclass[preprint,11pt]{sigplanconf}
\usepackage[colorlinks=true,linkcolor=black,urlcolor=black,citecolor=black]%
{hyperref}
\renewcommand{\t}{\texttt}
\renewcommand{\b}{\textbf}
\renewcommand{\i}{\textit}

\newcommand{\prettybox}[1]{\fbox{\parbox{\linewidth}{#1}}}

% A bunch of crazy BNF TeX I wrote once
\def\bnf#1{\prettybox{\halign{##\hfil& \hfil ##\hfil& \ ##\hfil\cr #1}}}
\def\p#1{& = & #1 \cr}
\def\pp#1{& & #1 \cr}
\def\a#1{& $\mid$ & #1 \cr}

\title{He-Man: A domain-specific language for lightweight concurrency}
%\subtitle{I said hey, what's goin' on}
\authorinfo{Carlo Angiuli}{}{cangiuli@cs.cmu.edu}
\authorinfo{Michael Sullivan}{}{mjsulliv@andrew.cmu.edu}
\date{December 12, 2011}

\begin{document}
\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Introduction}

Network applications need to support many simultaneous clients---this is known
as the C10K (``10,000 client'') problem \cite{Kegel}. Modern servers have enough
memory and bandwidth for massive concurrency, but it is difficult to design
applications to take full advantage of these resources.

A server must maintain separate state for each connection, and often waits for
network or disk I/O to complete before continuing to serve a particular client.
The simplest solution is to maintain one thread per client, using blocking I/O
within each thread. This incurs frequent and expensive context switches at the
operating system level; because the threads have identical code and little
private data, this approach is unnecessary and does not scale.

An improvement is to write multithreaded code, in which each thread serves
multiple clients. This is the most popular approach, and is utilized in the
Apache HTTP server \cite{Apache}.

Languages like Erlang and Haskell provide lightweight threads via user-space
schedulers. This affords even better performance, but remains unnecessarily
powerful for this problem space, imposing runtime overhead on applications which
don't require preemptable, independently scheduled threads \cite{Vinoski}.

By far the leanest approach is to write an event loop around kernel facilities
for multiplexed, non-blocking I/O, such as \t{select}, \t{poll}, \t{epoll}, or
\t{kquery}. In this model, all I/O occurs at a single call site in the code.
The programmer is responsible for explicitly maintaining per-client state and
invoking the appropriate client on each I/O event.

This requires the entire application to be restructured as a state machine with
explicit per-client control blocks, a model which is very difficult for
programmers to reason about. There exist numerous C/C++ libraries for event
loops, but none of these avoid the state machine paradigm.

Our observation is that the event loop model, in programming languages parlance,
is essentially trampolining continuation-passing-style, which can be obtained by
mechanical translation from the straight-line threaded code. This suggests it is
possible to obtain the best of both worlds---a threaded programming model with
the performance of event loops. 

To that end, we present He-Man: Haskell Event Manager, Apropos Networking, an
embedded domain-specific language (EDSL) hosted in Haskell, which provides a
threaded abstraction for network application programming and compiles to a C
event loop based around Linux's \t{epoll}.

He-Man is a straightforward monadic language with built-in network primitives,
offering a simple thread-per-client programming model. It is easily extensible
in Haskell, and also allows calls to foreign C functions; it outputs C code
which links with an event loop runtime and any code a programmer might prefer to
write directly in C. Our vision is that C programmers can use He-Man for their
core application logic, and can write the rest of their application in C. 

\section{Related work}

The Lighttpd \cite{Lighttpd} and Nginx \cite{Nginx} HTTP servers use event-based
architectures, with the stated goal of addressing the C10k problem. Indeed, they
are fast (XXX CITATION)

Haskell event loop stuff: \cite{LiZdancewic}

As mentioned in the intro,
Haskell has epoll support and lightweight threads
Erlang

\section{The event loop model}



Event loops are a good way to get the level of concurrency that you need

we don't need arbitrary interleaving of instructions, 


there shouldn't be much

and I/O time (both network and disk) probably dominates computation time

thus it's important to run short computations until a thread blocks, 

The back-end converts our linear front-end code into a collection of states 

is to compile C, 

It's important that the only blocking operations happen in the main loop

We separate 

Threads are separated and replaced by labels

pass to make it 



\section{Implementation}

\subsection{Front-end}

\begin{figure}[ht]
\centering
\bnf{
\i{Stmt}
\p{\b{Decl} (\i{Var}, \i{Type}) \i{Expr}}
\a{\b{While} \i{Expr} [\i{Stmt}]}
\a{\b{If} \i{Expr} [\i{Stmt}] [\i{Stmt}]}
\a{\b{Spawn} ([(\i{Var}, \i{Type})], [\i{Stmt}]) [\i{Expr}]}
\a{\b{Assign} \i{Expr} \i{Expr}}
\a{\b{Exp} \i{Expr}}
\a{\b{Wait} \i{Expr}}
\a{\b{Exit}}
\i{Expr}
\p{\b{Call} \i{String} [\i{Expr}]}
\a{\b{Arith} \i{ArithOp} \i{Expr} \i{Expr}}
\a{\b{ArithUnop} \i{ArithUnop} \i{Expr}}
\a{\b{RelnOp} \i{RelnOp} \i{Expr} \i{Expr}}
\a{\b{Constant} \i{String}}
\a{\b{NumLit} \i{Integer}}
\a{\b{StringLit} \i{String}}
\a{\b{Var} \i{String}}
\a{\b{CurThread}}
\i{Type} 
\p{\b{Int} $\mid$ \b{Bool} $\mid$ \b{String}
   $\mid$ \b{FD} $\mid$ \b{Buffer} $\mid$ \b{Event}}
}
\caption{Front-end grammar.}
\end{figure}

\subsection{Back-end}

The back-end converts our linear front-end code into a collection of states 

is to compile C, 

It's important that the only blocking operations happen in the main loop

We separate 

Threads are separated and replaced by labels

pass to make it 

\begin{figure}[ht]
\centering
\bnf{
\i{Thread}
\p{(\i{thread\_label}, [(\i{Var}, \i{Type})])}
\i{Block}
\p{(\i{label}, \i{thread\_label}, [\i{Stmt}], \i{Tail})}
\i{Tail}
\p{\b{Goto} \i{label}}
\a{\b{GotoWait} \i{label}}
\a{\b{If} \i{Expr} \i{Tail} \i{Tail}}
\a{\b{Exit}}
\i{Stmt}
\p{\b{Decl} (\i{Var}, \i{Type}) \i{Expr}}
\a{\b{Assign} \i{Expr} \i{Expr}}
\a{\b{Exp} \i{Expr}}
\a{\b{Spawn} \i{label} [((\i{Var}, \i{Type}), \i{Expr})]}
}
\caption{Back-end grammar.}
\end{figure}

\subsection{Runtime}

\section{Evaluation}

The per-thread memory overhead imposed by He-Man is very small: it
consists of a thread id, list pointers for the scheduling queue, the
next state to enter, a pointer to the most recently received event,
and list heads and tails for lists of the events being waited on,
events owned by the thread, and buffers owned by the thread (the
latter two are so that we can automatically reclaim these resources
when the thread exits). On a 64-bit system, the total overhead comes
out to 88 bytes, and could easily be reduced further (e.g. by using
singly linked lists).  For comparison, the default thread stack size
under NPTL on Linux is 2 megabytes.

say something about Memory.hs
each thread takes up XXX memory (see Memory.hs)

To evaluate He-Man,  

HTTP server performance is XXX

\section{Future work}

write a better allocator

less space per thread

decide when to allow internal if statements

We will additionally output a graphical representation of this underlying state
machine.

observable sharing (?)

\section{Conclusions}

\bibliography{citations}{}
\bibliographystyle{abbrvnat}

\end{document}
